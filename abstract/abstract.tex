\documentclass{article}

\usepackage[italian,english]{babel}
\usepackage[utf8x]{inputenc} %eventualmente da cambiare se ci sono problemi con accenti
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\usepackage[font=large, tablename= ]{caption}
\useunder{\uline}{\ul}{}

\setlength{\textwidth}{16 cm}
\setlength{\oddsidemargin}{0 cm}
\setlength{\topmargin}{-1.5 cm}
\setlength{\textheight}{30 cm}
\begin{document}

\begin{table}
\centering
\caption*{Optimization Techniques of Deep Learning Models for Visual Quality Improvement}
\begin{tabular}{lll}
\textbf{Candidate:}   & Lorenzo Palloni & \href{mailto:lorenzo.palloni@stud.unifi.it}{\texttt{lorenzo.palloni@stud.unifi.it}} \\
\textbf{Supervisor:}    & Marco Bertini & \href{mailto:marco.bertini@unifi.it}{\texttt{marco.bertini@unifi.it}} \\
\textbf{Co-supervisor:}    & Leonardo Galteri & \href{mailto:leonardo.galteri@unifi.it}{\texttt{leonardo.galteri@unifi.it}} \\
\textbf{Co-supervisor:}    & Donatella Merlini & \href{mailto:donatella.merlini@unifi.it}{\texttt{donatella.merlini@unifi.it}} \\

\end{tabular}
\end{table}
\subsection*{Thesis summary}

This thesis examines the efficacy of quantization techniques for enhancing the inference speed and reducing the memory usage of deep learning models applied to video restoration tasks. The research investigates the implementation and evaluation of post-training quantization using TensorRT. The results indicate that reducing the precision of weights and activations substantially decreases computational complexity and memory requirements without compromising performance. In particular, the INT8-optimized UNet and SRUNet models achieve 2.38X and 2.26X speedup compared to their plain implementations, respectively, while also achieving memory consumption reductions of 63.3\% for UNet and 53.8\% for SRUNet. These findings contribute to the development of more practical and efficient video restoration models for real-world applications.

\end{document}
