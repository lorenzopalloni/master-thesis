\section{LPIPS-Comp}
\label{sec:lpipscomp}
The same technique used to train LPIPS has been adopted for LPIPS-Comp. While LPIPS is trained on BAPPS, that contains images with several distortions but accounts only for compression artefacts from JPEG, LPIPS-Comp has seen a compression specific perceptual similarity dataset. In doing so, experiments showed that LPIPS-Comp aligns more to human judgement than the standard LPIPS on general compression tasks.

LPIPS-Comp \cite{patel2021saliency} is a perceptual similarity metric based on deep neural networks obtained following the same approach as in \cite{zhang2018unreasonable} with LPIPS.
These methods employ the $ReLU$ activations after each \textit{conv} block in the first five layers of the VGG-16 \cite{simonyan2014very} architecture, with batch-normalization \cite{ioffe2015batch}.

Feed-forward is performed on VGG-16 for both the original ($y$) and the reconstructed image ($\hat{y}$).
Let $L$ be the set of layers used for loss calculation (five for our setup), a function $F(y)$ denoting feed-forward on an input image $y$.
$F(y)$ and $F(\hat{y})$ return two stacks of feature activationâ€™s for all $L$ layers.

The LPIPS-Comp loss is then computed as:
\begin{itemize}
\item $F(y)$ and $F(\hat{y})$ are unit-normalized in the channel dimension. Let us call these, $z^l_y, z^l_{\hat{y}} \in R^{H_l \times W_l \times C_l}$ where $l \in L$. ($H_l, W_l$ are the spatial dimensions).
\item $z^l_y , z^l_{\hat{y}}$ are scaled channel wise by multiplying with the vector $w_l \in R^{C_l}$.
\item The $L_2$ distance is then computed and an average over spatial dimension is taken. Finally, a channel-wise sum is performed.
\end{itemize}

Equation \ref{eq:lpips-comp} summarizes the LPIPS-Comp computation.

\begin{align}
\text{LPIPS-Comp}(y, \hat{y}) = \sum_{l} \frac{1}{H_l W_l} \sum_{h, w} \left| \left| w_l \odot \left( z^l_{\hat{y}, h, w} - z^l_{y, h, w} \right) \right| \right|^2_2\label{eq:lpips-comp}
\end{align}

Note that the weights in $F$ are learned for image classification on the ImageNet dataset \cite{russakovsky2015imagenet} and are kept fixed. $w$ are the linear weights learned on top of $F$ on the BAPPS \cite{zhang2018unreasonable} dataset for LPIPS and on a compression specific similarity dataset for LPIPS-Comp. While in the first, compressed images are obtained with JPEG only, the second makes use of several compression methods: Mentzer et al. \cite{mentzer2018conditional}, Patel et al. \cite{patel2019deep}, BPG \cite{bellard2014bpg} and JPEG-2000 \cite{skodras2001jpeg}.
