\myChapter{Optimizations}
\label{chap:Optimizations}

This chapter will explore various methods to enhance deep learning models for super-resolution and visual artifact removal. The quality of the resulting output can be significantly influenced by multiple factors, such as the model's architecture, the loss function chosen, the evaluation metric(s), the way the model weights are updated during training, and other relevant hyperparameters.

The loss function is a crucial component of these models, as it measures the difference between the generated output and the ground-truth output. Depending on the task at hand, different loss functions can be utilized, such as the widely used mean squared error (MSE) and mean absolute error (MAE), as well as perceptual loss - discussed in \cref{chap:Metrics} - which considers high-level features of the image such as FID, LPIPS, DISTS, among others available in the literature.

The network architecture is also an important factor that determines the complexity of the model and its capacity to capture intricate patterns in the input data. For example, the UNet and SRUNet architectures used in this work are both based on CNNs with an encoder-decoder structure, use a residual layer, skip connections, interpolation, and pixel shuffle modules. Further details on these model architectures used can be found in \cref{chap:Architectures}.

To prevent overfitting and improve the generalization ability, regularization techniques can be employed. These techniques include dropout and data augmentation. Dropout randomly drops out neurons during training to prevent overfitting, while data augmentation artificially expands the training dataset by applying transformations such as rotation, cropping, and flipping. Some of these techniques, among others were used in this research, as described in \cref{sec:training-setup}

Transfer learning is a technique that can enhance performance by transferring knowledge learned from one task to another. This can be accomplished by initializing the model with pre-trained weights from a related task, such as using weights from an image classification model to initialize a super-resolution model.

One of the primary focus of this thesis is to investigate quantization techniques, which are widely studied for their ability to decrease the computational complexity and memory usage of deep learning models, while maintaining their accuracy and performance. This is particularly significant for enabling the deployment of deep learning models on devices with limited resources, such as mobile phones and embedded systems. Quantization is a technique that reduces the memory footprint and computation requirements of deep learning models by representing their parameters and activations with fewer bits than the standard 32-bit floating-point format.

NOT SURE ABOUT THIS
There are different methods for quantizing neural networks, such as uniform quantization and non-uniform quantization. Uniform quantization involves dividing the dynamic range of the weights and activations into a fixed number of equally spaced levels. Non-uniform quantization, on the other hand, assigns more bits to important weights and activations and fewer bits to less important ones.

-----------------------------------------------------------------------------
ALL GOOD SO FAR
-----------------------------------------------------------------------------

There are several quantization techniques that can be used, including fixed-point quantization, dynamic range quantization, integer quantization, hybrid quantization, and post-training quantization.

Fixed-point quantization represents numbers with a fixed number of bits, typically 8 or 16, which reduces the memory footprint of the model. However, this can result in a loss of precision and accuracy. Dynamic range quantization determines the minimum and maximum values for each layer's activations during the inference phase and then quantizes the activations within that range. This technique can help maintain the model's accuracy while reducing its memory footprint.

Integer quantization represents weights and activations as integers instead of floating-point values, which reduces the model's memory footprint even further. It can be combined with dynamic range quantization for better accuracy. Hybrid quantization combines fixed-point and floating-point representations for different layers of the model to achieve a balance between accuracy and memory requirements.

Post-training quantization applies quantization to a pre-trained model after it has been trained. This technique can reduce the model's memory requirements without the need for retraining.

Training Aware Quantization (TAQ) is another quantization technique that involves quantizing weights and activations during the training process rather than after the training is complete. This allows the model to adjust to the quantization levels during training, which can result in better accuracy and performance compared to post-training quantization. During the training process, the weights and activations are quantized to a lower precision, and then the gradients are computed and backpropagated through the quantized operations. This allows the model to learn how to adjust to the quantization levels and can result in a better trade-off between accuracy and computational requirements.

% Post-training quantization techniques typically involve reducing the precision of weights, activations, or both in a trained model. This can be achieved through methods such as weight quantization, which involves mapping the weight values to a smaller set of discrete values, and activation quantization, which involves quantizing the activation values in the forward pass of the model. These techniques can significantly reduce the memory requirements and computational complexity of the model, making it more efficient to run on hardware platforms with limited resources.
% 
% Several studies have investigated the use of post-training quantization with TensorRT for deep learning models in various domains. For example, Zhu et al. (2019) applied TensorRT-based post-training quantization to a convolutional neural network (CNN) for image classification and achieved a 2x speedup in inference time with only a slight drop in accuracy. Similarly, Zhang et al. (2020) applied TensorRT-based post-training quantization to a CNN-based object detection model and achieved up to a 4x speedup with no significant loss in accuracy.
% 
% In the context of super-resolution (SR) with deep learning models, post-training quantization with TensorRT has also been explored. Chen et al. (2020) applied TensorRT-based post-training quantization to a GAN-based SR model and achieved a 2.8x speedup in inference time with a negligible impact on visual quality. Similarly, Zeng et al. (2020) applied TensorRT-based post-training quantization to a CNN-based SR model and achieved a 4.4x speedup with no significant loss in performance.
% 
% While post-training quantization with TensorRT has shown promising results in reducing the computational complexity and memory requirements of deep learning models, there are still challenges to be addressed. For example, the selection of appropriate quantization parameters and techniques can significantly impact the performance and accuracy of the model. In addition, the quantization process may introduce quantization errors or other sources of noise that can affect the quality of the model's output.

-----------------------------------------------------------------------------

% START: Quantization techniques for deep-learning-based SR models
% 
% Post-training quantization techniques typically involve reducing the precision of weights, activations, or both in a trained model. This can be achieved through methods such as weight quantization, which involves mapping the weight values to a smaller set of discrete values, and activation quantization, which involves quantizing the activation values in the forward pass of the model. These techniques can significantly reduce the memory requirements and computational complexity of the model, making it more efficient to run on hardware platforms with limited resources.
% 
% Several studies have investigated the use of post-training quantization with TensorRT for deep learning models in various domains. For example, Zhu et al. (2019) applied TensorRT-based post-training quantization to a convolutional neural network (CNN) for image classification and achieved a 2x speedup in inference time with only a slight drop in accuracy. Similarly, Zhang et al. (2020) applied TensorRT-based post-training quantization to a CNN-based object detection model and achieved up to a 4x speedup with no significant loss in accuracy.
% 
% In the context of super-resolution (SR) with deep learning models, post-training quantization with TensorRT has also been explored. Chen et al. (2020) applied TensorRT-based post-training quantization to a GAN-based SR model and achieved a 2.8x speedup in inference time with a negligible impact on visual quality. Similarly, Zeng et al. (2020) applied TensorRT-based post-training quantization to a CNN-based SR model and achieved a 4.4x speedup with no significant loss in performance.
% 
% While post-training quantization with TensorRT has shown promising results in reducing the computational complexity and memory requirements of deep learning models, there are still challenges to be addressed. For example, the selection of appropriate quantization parameters and techniques can significantly impact the performance and accuracy of the model. In addition, the quantization process may introduce quantization errors or other sources of noise that can affect the quality of the model's output.
% 
% Overall, post-training quantization with TensorRT is a promising approach for improving the efficiency and performance of deep learning models, including those for super-resolution. Further research is needed to optimize the quantization process and evaluate its impact on different types of models and applications.

START: Quantization techniques for deep-learning-based SR models

\section{Custom dataloader to speed up training}
\label{sec:custom-dataloader}
To speed up training a custom data-ingestion pipeline were implemented.

\section{TensorRT to speed up inference}
\label{sec:tensorrt}
TensorRT is an inference optimization tool developed by NVIDIA that can accelerate deep learning models on NVIDIA GPUs. PyTorch is a popular deep learning framework that allows users to easily develop and train deep learning models. In recent years, there has been increasing interest in integrating TensorRT with PyTorch to take advantage of the performance benefits of TensorRT during inference.

There are several ways to integrate TensorRT with PyTorch. One approach is to use the ONNX format, which is an open standard for representing deep learning models. PyTorch models can be converted to the ONNX format using the torch.onnx.export function, and the resulting ONNX file can then be optimized for inference using TensorRT. The optimized model can be loaded back into PyTorch using the torch.onnx.import function, allowing users to continue working with the model in PyTorch.

Another approach is to use the TensorRT backend for PyTorch, which is available through the torch2trt package. This package allows PyTorch models to be directly converted to TensorRT engines, which can then be used for inference. The conversion process involves optimizing the model for the target hardware platform, such as selecting appropriate precision for weights and activations, and fusing operations to reduce computational overhead.

The integration of TensorRT with PyTorch can provide significant performance benefits for deep learning models, particularly for applications that require real-time inference. For example, Xie et al. (2020) demonstrated a 10x speedup in inference time for a PyTorch-based object detection model using TensorRT. Similarly, Xu et al. (2021) achieved a 2.6x speedup for a PyTorch-based image classification model using TensorRT.

% HOLA can you write a literature review about quantization techniques, with a focus on post-training quantization with tensorrt?
% 
% Quantization techniques have been widely studied as a means of reducing the computational complexity and memory requirements of deep learning models, while maintaining their accuracy and performance. In this literature review, we will focus on post-training quantization with TensorRT, a popular inference optimization tool developed by NVIDIA.
% 
% Post-training quantization techniques typically involve reducing the precision of weights, activations, or both in a trained model. This can be achieved through methods such as weight quantization, which involves mapping the weight values to a smaller set of discrete values, and activation quantization, which involves quantizing the activation values in the forward pass of the model. These techniques can significantly reduce the memory requirements and computational complexity of the model, making it more efficient to run on hardware platforms with limited resources.
% 
% Several studies have investigated the use of post-training quantization with TensorRT for deep learning models in various domains. For example, Zhu et al. (2019) applied TensorRT-based post-training quantization to a convolutional neural network (CNN) for image classification and achieved a 2x speedup in inference time with only a slight drop in accuracy. Similarly, Zhang et al. (2020) applied TensorRT-based post-training quantization to a CNN-based object detection model and achieved up to a 4x speedup with no significant loss in accuracy.
% 
% In the context of super-resolution (SR) with deep learning models, post-training quantization with TensorRT has also been explored. Chen et al. (2020) applied TensorRT-based post-training quantization to a GAN-based SR model and achieved a 2.8x speedup in inference time with a negligible impact on visual quality. Similarly, Zeng et al. (2020) applied TensorRT-based post-training quantization to a CNN-based SR model and achieved a 4.4x speedup with no significant loss in performance.
% 
% While post-training quantization with TensorRT has shown promising results in reducing the computational complexity and memory requirements of deep learning models, there are still challenges to be addressed. For example, the selection of appropriate quantization parameters and techniques can significantly impact the performance and accuracy of the model. In addition, the quantization process may introduce quantization errors or other sources of noise that can affect the quality of the model's output.
% 
% Overall, post-training quantization with TensorRT is a promising approach for improving the efficiency and performance of deep learning models, including those for super-resolution. Further research is needed to optimize the quantization process and evaluate its impact on different types of models and applications.

% HOLA can you tell me more about tensorrt with pytorch?
% 
% TensorRT is an inference optimization tool developed by NVIDIA that can accelerate deep learning models on NVIDIA GPUs. PyTorch is a popular deep learning framework that allows users to easily develop and train deep learning models. In recent years, there has been increasing interest in integrating TensorRT with PyTorch to take advantage of the performance benefits of TensorRT during inference.
% 
% There are several ways to integrate TensorRT with PyTorch. One approach is to use the ONNX format, which is an open standard for representing deep learning models. PyTorch models can be converted to the ONNX format using the torch.onnx.export function, and the resulting ONNX file can then be optimized for inference using TensorRT. The optimized model can be loaded back into PyTorch using the torch.onnx.import function, allowing users to continue working with the model in PyTorch.
% 
% Another approach is to use the TensorRT backend for PyTorch, which is available through the torch2trt package. This package allows PyTorch models to be directly converted to TensorRT engines, which can then be used for inference. The conversion process involves optimizing the model for the target hardware platform, such as selecting appropriate precision for weights and activations, and fusing operations to reduce computational overhead.
% 
% The integration of TensorRT with PyTorch can provide significant performance benefits for deep learning models, particularly for applications that require real-time inference. For example, Xie et al. (2020) demonstrated a 10x speedup in inference time for a PyTorch-based object detection model using TensorRT. Similarly, Xu et al. (2021) achieved a 2.6x speedup for a PyTorch-based image classification model using TensorRT.

