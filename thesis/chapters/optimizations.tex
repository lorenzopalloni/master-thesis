\myChapter{Optimizations}
\label{chap:Optimizations}

This chapter will explore various methods to enhance deep learning models for super-resolution and visual artefact removal. The quality of the resulting output can be significantly influenced by multiple factors, such as the model's architecture, the loss function chosen, the evaluation metric(s), the way the model weights are updated during training, and other relevant hyperparameters.

The loss function is a crucial component of these models, as it measures the difference between the generated output and the ground-truth output. Depending on the task at hand, different loss functions can be utilized, such as the widely used mean squared error (MSE) and mean absolute error (MAE), as well as perceptual loss - discussed in \cref{chap:Metrics} - which considers high-level features of the image such as FID, LPIPS, DISTS, among others available in the literature.

The network architecture is also an important factor that determines the complexity of the model and its capacity to capture intricate patterns in the input data. For example, the UNet and SRUNet architectures used in this work are both based on CNNs with an encoder-decoder structure, use residual layers, skip connections, interpolation and pixel shuffle modules. Further details on these model architectures can be found in \cref{chap:Architectures}.

To prevent overfitting and improve generalization ability, regularization techniques can be employed. These techniques include dropout and data augmentation. Dropout randomly drops out neurons during training to prevent overfitting, while data augmentation artificially expands the training dataset by applying transformations such as rotation, cropping, and flipping. Some of these techniques, among others were used in this research, as described in \cref{sec:training-setup}

\section{Quantization}
\label{sec:quantization}

Another approach to optimise deep learning models, and one of the primary focuses of this thesis, is quantization. Quantization techniques are widely studied for their ability to decrease the computational complexity and memory usage of deep learning models while maintaining their accuracy and performance. This is particularly significant for enabling the deployment of deep learning models on devices with limited resources, such as mobile phones and embedded systems. Quantization is a technique that reduces the memory footprint and computation requirements of deep learning models by representing their parameters and activations with fewer bits than the standard 32-bit floating-point format.

There are different methods for quantizing neural networks, such as uniform quantization and non-uniform quantization. Uniform quantization involves dividing the dynamic range of the weights and activations into a fixed number of equally spaced levels. Non-uniform quantization, on the other hand, assigns more bits to important weights and activations and fewer bits to less important ones allowing for more precise capture of signal information. However, implementing non-uniform quantization schemes on common computational hardware, such as GPUs and CPUs, can be challenging. Therefore, the current standard method for quantization is uniform quantization due to its simplicity and efficient mapping to hardware \cite{gholami2021survey}.

Another way in which quantization methods can be divided is in fixed-point, integer, and hybrid quantization. Fixed-point quantization represents numbers with a fixed number of bits, typically 8 or 16, which reduces the memory footprint of the model. Integer quantization represents weights and activations as integers instead of floating-point values, which reduces the model's memory footprint even further. Hybrid quantization combines fixed-point and floating-point representations for different layers of the model to achieve a balance between accuracy and memory requirements.

It is often necessary to fine-tune the parameters in a neural network after quantization. There are two ways to accomplish this: one approach involves re-training the model, which is known as Quantization-Aware Training (QAT), while the other approach is to modify the parameters without re-training, a method commonly referred to as Post-Training Quantization (PTQ).

QAT involves re-training a neural network model with quantized parameters to address the perturbation introduced by quantization, which may shift the model away from its convergence point in floating point precision. During QAT, forward and backward passes are performed in floating point on the quantized model, with the model parameters being quantized after each gradient update using projected gradient descent. It is crucial to perform this projection after the weight update in floating point precision to maintain accuracy. Back-propagation in floating point is also essential to avoid zero-gradient or gradients with high errors, which can occur when accumulating gradients in quantized precision, particularly in low-precision scenarios. The non-differentiable quantization operator is approximated using the Straight Through Estimator (STE) to address this issue. Despite being a coarse approximation, STE typically works well in practice except for ultra-low-precision quantization such as binary quantization.

Although QAT has been demonstrated to be effective, the main drawback is the computational cost of re-training the neural network model. This re-training may take several hundred epochs to recover accuracy, particularly in low-bit precision quantization. If a quantized model is expected to be deployed for a long time and efficiency and accuracy are critical, then the investment in re-training is likely worthwhile. However, this may not be the case for all models, particularly those with a short lifetime. An alternative approach that avoids this overhead is discussed in the following section.

During PTQ, the quantization parameters for both weights and activations are established without performing any re-training on the neural network model. Consequently, PTQ is a rapid technique for quantizing neural network models. However, this approach often results in lower accuracy compared to QAT.

\section{TensorRT to speed up inference}
\label{sec:tensorrt}

TensorRT is an inference optimization tool developed by NVIDIA that can accelerate deep learning models on NVIDIA GPUs. In recent years, there has been increasing interest in integrating TensorRT with PyTorch \cite{NEURIPS2019_9015}, one of the most popular deep learning frameworks that allows users to easily develop and train deep learning models, to take advantage of the performance benefits of TensorRT during inference.
 
There are several ways to integrate TensorRT with PyTorch. One approach is to use the ONNX \cite{bai2019} format, which is an open standard for representing deep learning models. PyTorch models can be converted to the ONNX format using the \texttt{torch.onnx.export} function and the resulting ONNX file can then be optimized for inference using TensorRT. The optimized model can be loaded back into PyTorch using the \texttt{torch.onnx.import} function, allowing users to continue working with the model in PyTorch.

Another approach is to use Torch-TensorRT \cite{torchtensorrt}. Torch-TensorRT is a compiler that is designed for PyTorch, TorchScript, and FX, with the aim of optimizing them for NVIDIA GPUs through the use of NVIDIA's TensorRT.

TorchScript is a scripting language and runtime environment for PyTorch that allows you to serialize and save PyTorch models and run them in production without needing the full PyTorch framework. It enables the execution of PyTorch models efficiently on a wide range of devices.

FX is a new, experimental toolkit for building high-performance machine learning models in PyTorch. It allows developers to compose models using pure Python syntax and provides tools for optimizing and deploying those models on various hardware platforms. FX is designed to enable fast experimentation and easy production deployment of PyTorch models.

Unlike PyTorch's Just-In-Time (JIT) compiler, Torch-TensorRT is an Ahead-of-Time (AOT) compiler. This means that before deploying your TorchScript code, you must go through an explicit compile step to convert a standard TorchScript or FX program into a module targeting a TensorRT engine. Torch-TensorRT works as a PyTorch extension, and compiled modules can integrate seamlessly into the JIT runtime. Once compiled, the optimized graph should feel no different from running a TorchScript module. Additionally, you have access to TensorRT's suite of configurations at compile time, allowing you to specify operating precision (FP32/FP16/INT8) and other settings for your module.

The Torch-TensorRT Python API offers a simple and user-friendly approach to utilizing PyTorch data-loaders in conjunction with TensorRT calibrators. By specifying the desired configuration, the DataLoaderCalibrator class can be employed to establish a TensorRT calibrator.

Several studies have investigated the use of TensorRT to improve the inference performance of deep learning models in various domains. For example, St\"acker et al. \cite{stacker2021deployment} applied TensorRT-based post-training quantization to representative object detection networks, such as RetinaNet for image-based 2D object detection and PointPillars for LiDAR-based 3D object detection. The authors focused on deploying these networks on an edge AI platform, examining the conversion process from the PyTorch training environment to the deployment environment, and comparing the performance of TensorRT and TorchScript. Their experiments demonstrated slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers. Furthermore, they analyzed the trade-off between runtime and performance when optimizing the deployment setup, finding that quantization significantly reduced runtime while having a minimal impact on detection performance.

In the experiments conducted throughout this thesis, Post-Training Quantization was utilized as the chosen quantization method, employing the Torch-TensorRT framework. This approach allowed for the quantization of the UNet and SRUNet without re-training, resulting in a more efficient and expedient process.

\section{Data-loader to speed up training}
\label{sec:dataloader}

In this section is presented a novel and custom data-loading pipeline implemented for training super-resolution models. Its primary goal is to reduce computational bottlenecks arising from four interrelated factors: GPU RAM capacity, GPU batch processing time, CPU image fetching time, and image size. The models were trained on an Ubuntu 20.04 server using a single GPU Titan Xp with 12GB RAM. The BVI-DVC dataset is stored on an external HDD, which cannot be transferred to an SSD due to its size.

Initially, a naive PyTorch data loader was implemented to load one image at a time, extracting only one patch from each loaded image. However, this approach led to a significant bottleneck for the GPU as the CPU's processing time exceeded that of the GPU's forward/backward pass. A more efficient solution would be to store patches from each frame (or a subset of frames) in SSDs near the CPU, allowing parallel processing to prevent GPU bottlenecks.

To overcome these challenges, the custom data-loading pipeline comprises two main components: the BufferGenerator and the BatchGenerator. The BufferGenerator serves buffered chunks of items from a list and applies a function to each item before it enters the buffer. The BatchGenerator generates batches of patches taken from prefetched frames, with both image selection and cropping taking place randomly.

The pipeline also provides different behaviours for the training, validation, and testing stages. In the training stage, the data is shuffled, while in the validation and testing stages, it remains in its original order. The custom data-loading pipeline ensures efficient training of super-resolution models without causing GPU bottlenecks or excessive memory consumption.

