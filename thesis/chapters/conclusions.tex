\myChapter{Conclusions}
\label{chap:Conclusions}

In this study, the integration of post-training quantization techniques was investigated to optimize deep learning models for super-resolution during inference. The results indicate that reducing the precision of weights and activations in both the UNet and SRUNet models substantially decreases computational complexity (up to 2.38X speedup) and memory requirements (up to 63.3\% of size reduction) without compromising performance, rendering them more practical and cost-effective for real-world applications, where real-time inference is often required.

The use of TensorRT, a powerful inference optimization tool developed by NVIDIA, also played a key role in achieving these results. By integrating TensorRT with PyTorch, we were able to take advantage of its performance benefits on NVIDIA GPUs, further improving the efficiency of the model.

Although our findings show promising results, there are still areas for future research. For example, other optimization techniques such as pruning or weight sharing could be explored to further reduce the computational complexity and memory requirements of deep learning models. In addition, the impact of different quantization parameters and techniques could be investigated in greater detail.

It is hoped that these findings will stimulate additional advancements and applications for future research in optimizing deep learning models for video quality improvement.
