\myChapter{Conclusions}
\label{chap:Conclusions}

In this study, the integration of post-training quantization techniques was investigated to optimize deep learning models for super-resolution during inference. The results indicate that reducing the precision of weights and activations in both the UNet and SRUNet models substantially decreases computational complexity (up to 2.38X speedup) and memory requirements (up to 63.3\% of size reduction) without compromising performance, rendering them more practical and cost-effective for real-world applications, where real-time inference is often required.

The use of TensorRT, a powerful inference optimization tool developed by NVIDIA, also played a key role in achieving these results. By integrating TensorRT with PyTorch, the efficiency of the model was further improved taking advantage of the INT8 computational capabilities of recent NVIDIA GPUs.

The outcomes of this thesis show promising results, and there are still areas for future research. For example, other optimization techniques such as pruning, weight sharing or model distillation could be explored to further reduce the computational complexity and memory requirements of deep learning models. In addition, the impact of different quantization parameters and techniques could be investigated in greater detail.

It is hoped that these findings will stimulate additional advancements and applications for future research in optimizing deep learning models for video quality improvement.

