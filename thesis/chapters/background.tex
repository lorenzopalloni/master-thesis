\myChapter{Background}
\label{chap:Background}

In this chapter, the existing literature on methods to address the video restoration task is reviewed, with a focus on recent deep-learning models for single-image super-resolution that are closely related to this thesis.

Dong et al. \cite{dong2014learning} present the SRCNN model using convolutional neural networks for single-image super-resolution, while Kim et al. introduce VDSR \cite{kim2016accurate} and DRCN \cite{kim2016deeply} models with a residual learning framework \cite{he2016deep}. Dong et al. \cite{dong2016accelerating} and Shi et al. \cite{shi2016real} efficiently map low-resolution to high-resolution images using deconvolution and sub-pixel convolution layers, respectively. LatticeNet \cite{luo2020latticenet}, LapSRN \cite{lai2017deep}, MemNet \cite{tai2017memnet}, SRDenseNet \cite{tong2017image}, and RDN \cite{zhang2018residual} are other lightweight and efficient SR models. Haris et al. \cite{haris2018deep} and DSRN \cite{han2018image} focus on iterative up-sampling and down-sampling and dual-state recurrent networks, respectively. MSRN \cite{li2018multi} and RFA \cite{liu2020residual} exploit image features efficiently by using different blocks, while recently attention mechanisms have also been used to improve super-resolution image quality \cite{dai2019second, mei2020image, niu2020single, zhang2018image}.

In recent years, SR models based on a generative adversarial network (GAN) framework have also gained popularity. SRGAN \cite{ledig2017photo} is a notable example, in which the generator network is trained to produce high-resolution images that are perceptually similar to the ground truth images, while the discriminator network is trained to distinguish between the generated and ground truth images. Since then, several GAN-based SR models have been proposed, including EnhancedNet \cite{sajjadi2017enhancenet}, ESRGAN \cite{wang2018esrgan}, SPSR \cite{ma2020structure}, SRFBN \cite{li2019feedback}, and SRFlow \cite{lugmayr2020srflow}.

In \cite{galteri2017deep}, Galteri et al. use adversarial training to remove artefacts from lossy compression algorithms for images and videos, training models with larger batches and utilizing an ensemble of networks. They also address real-time artefact removal using MobileNetV2-inspired generators with depth-wise separable convolutions \cite{galteri2019towards, galteri2019fast}. Mameli et al. \cite{mameli2020image} apply the no-GAN approach for compression artefact removal and super-resolution, using pre-training and a few GAN training iterations. Kaneko et al. introduce BNCR-GAN \cite{kaneko2021blur}, an architecture that removes noise, compression artefacts, and blurring artefacts by combining three models and using adaptive consistency losses.

Pourreza et al. \cite{pourreza2019recognizing} propose a GAN-based quality enhancement method to address accuracy loss in action recognition tasks caused by video compression, using a frame-recurrent strategy. Yu et al. develop VR-GAN \cite{yu2019hevc} for HEVC compression-generated artefact removal, operating at the inter-frame level and using flow estimation for coherence. In \cite{wang2020visual}, He et al. enhance HEVC compressed videos using adversarial loss combined with L1 loss, employing a residual network generator.

In \cite{galteri2020increasing}, Galteri et al. present a full pipeline architecture featuring semantic deep encoding and decoding, with a semantic mask for each frame to allocate more bits to semantically interesting content. The decoding side uses a Relativistic GAN and a segmentation-aware loss.

Video super-resolution poses a greater challenge than single-image super-resolution because it requires accurate prediction of both content and motion. Motion information is crucial for reconstructing high-resolution frames from several low-resolution images. The most effective multi-image super-resolution methods include sliding-window techniques and recurrent approaches, both of which can potentially restore more high-resolution details in target frames as they utilize more visual information. However, these methods must consider motion content between frames to achieve top-quality super-resolution results.

Recurrent neural networks, widely used for various vision tasks like classification, detection, and segmentation, can process input of any length by sharing model weights over time. These recurrent models can account for long-range dependencies among pixels, making them well-suited for video super-resolution tasks. In recent years, many video super-resolution models based on recurrent neural networks have been developed, demonstrating their effectiveness in this field.
