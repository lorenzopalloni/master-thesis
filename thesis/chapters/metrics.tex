\myChapter{Metrics}
\label{chap:Metrics}

To evaluate the performance of deep learning models for video restoration, various metrics have been proposed in the current literature, which can be broadly categorized into traditional and perceptual metrics.

\section{Traditional Metrics}
\label{sec:traditional-metrics}
Traditional metrics are based on simple numerical comparisons between the generated and ground-truth images. Some commonly used traditional metrics include PSNR, MSE, SSIM \cite{wang2004image}, and MS-SSIM \cite{wang2003multiscale}.

PSNR (Peak Signal-to-Noise Ratio) is a widely used metric that measures the ratio of the peak signal power to the noise power in an image. It is calculated as the logarithm of the ratio of the maximum possible pixel value to the mean squared error between the predicted and ground-truth images. However, PSNR has been criticized for not being a reliable measure of image quality, as it does not correlate well with human perception.

MSE (Mean Squared Error) measures the average squared difference between the predicted and ground-truth images, with lower values indicating better image quality. However, like PSNR, it has been found to correlate poorly with human perception \cite{girod1993s,wang2009mean}.

SSIM (Structural SIMilarity) is a more sophisticated metric that takes into account both structural information and pixel values in the image. It measures the similarity between the predicted and ground-truth images based on their luminance, contrast, and structure, and has been found to correlate better with human perception than PSNR and MSE.

MS-SSIM (Multi-Scale SSIM) is an improved version of the SSIM metric that takes into account the multi-scale nature of the human visual system.

\section{Perceptual Metrics}
\label{sec:perceptual-metrics}
Perceptual metrics aim to evaluate image quality based on human perception, measuring the visual similarity between predicted and ground-truth images, rather than simply their pixel-wise differences. Examples of deep learning-based perceptual metrics include FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, LPIPS-Comp \cite{patel2021saliency}, E-LPIPS \cite{kettunen2019lpips}, and DISTS \cite{ding2020image}.

FID (Fr√©chet Inception Distance) is a perceptual metric used to evaluate the similarity between two sets of images by measuring the distance between their feature representations obtained from a pre-trained neural network.

LPIPS (Learned Perceptual Image Patch Similarity) computes the similarity between two images based on their perceptual similarity at the patch level, using a deep neural network trained on human perceptual judgments.

LPIPS-Comp (LPIPS with Saliency Map Comparison) is an extension of LPIPS that incorporates saliency maps to enhance the metric sensitivity to the most salient regions in the image.

E-LPIPS (Ensembled LPIPS) is an improved version of the LPIPS metric that employs an ensemble of neural networks trained on different subsets of images to improve the stability and robustness of the metric.

DISTS (Deep Image Structure and Texture Similarity) is based on a Siamese neural network, which takes two input images, and extracts features from them. These features are then compared at multiple levels to compute a final similarity score between the two images.

Other perceptual metrics such as MOS (Mean Opinion Score), 2AFC (Two Alternative Forced Choice), and JND (Just Noticeable Difference) are all examples of perceptual metrics that are based on subjective human evaluations of image quality.

MOS involves asking human subjects to rate the quality of the predicted images on a scale from 1 to 5, with the MOS score calculated as the average of these ratings. 2AFC involves presenting two images to human subjects and asking them to choose the one that appears to be of higher quality, while JND involves asking subjects to identify the minimum perceptible difference between two images.

Overall, the choice of metric for evaluating super-resolution models depends on the specific application and the goals of the study. Traditional metrics such as PSNR, MSE, and SSIM are simple to compute and provide a good baseline for comparison. Perceptual metrics such as LPIPS and MOS provide a more accurate measure of human perception but are more complex to compute and require additional resources. A combination of both traditional and perceptual metrics can provide a comprehensive evaluation of the performance of super-resolution models.


\section{No-Reference Metrics}
\label{sec:no-reference-metrics}
While Full-Reference Image Quality Assessment (FR-IQA) measures have been discussed thus far, it is worth noting that No-Reference IQA (NR-IQA) metrics are also commonly employed to assess processed images without their original counterparts. Examples of such NR-IQA metrics include BRISQUE \cite{mittal2012no}, NIQE \cite{mittal2012making}, PIQUE \cite{venkatanath2015blind}, and CONTRIQUE \cite{madhusudana2022image}.

BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator) is an NR-IQA metric that relies on a spatial Natural Scene Statistics (NSS) model \cite{ruderman1994statistics} of locally normalized luminance coefficients in the spatial domain, as well as the model for pairwise products of these coefficients. BRISQUE predicts the image quality score by using a support vector regression model trained on an image database with corresponding Differential Mean Opinion Score (DMOS) values. DMOS is the difference between the perfect quality score and the MOS. Thus, a higher MOS value and a lower DMOS value indicate better quality. The database contains images with known distortion such as compression artefacts, blurring, and noise, and it contains pristine versions of the distorted images.

NIQE (Natural Image Quality Evaluator) computes the distance between the NSS-based features calculated from a given image to the features obtained from an image database used to train the model. The features are modelled as multidimensional Gaussian distributions.

PIQUE (Perception-based Image QUality Evaluator) uses an opinion-unaware methodology that does not require any training data. It extracts local features to predict quality and estimates quality only from perceptually significant spatial regions. PIQUE can generate a fine-grained block-level distortion map and it has low computational complexity despite working at the block-level.

CONTRIQUE (CONTRastive Image QUality Evaluator) obtains image quality representations in a self-supervised manner. It tries to predict distortion type and degree as an auxiliary task to learn features from an unlabeled image dataset containing a mixture of synthetic and realistic distortions. A convolutional neural network is trained using a contrastive pairwise objective to solve the auxiliary problem. The network weights are frozen during evaluation and a linear regressor maps the learned representations to quality scores in a No-Reference setting.

Datasets like LIVE \cite{sheikh2006statistical}, TID2008 \cite{ponomarenko2009tid2008}, CSIQ \cite{larson2010most}, and TID2013 \cite{ponomarenko2013color} are considered FR-IQA datasets since they need a reference image to assess the image quality. In contrast, AVA \cite{murray2012ava} and LIVE In the Wild \cite{ghadiyaram2015massive} are NR-IQA datasets, where image quality is evaluated independently without the necessity of a reference image.

\section{Video Quality Metrics}
\label{sec:video-quality-metrics}

Some image quality metrics, such as PSNR or SSIM, can be used to assess the quality of a video, by considering one frame at a time. The quality measure for each frame can be recorded and averaged over time to determine the overall quality of the video. However, this approach does not take into account certain types of degradation that occur over time, such as moving artefacts caused by packet loss and its concealment. A video quality metric that takes into account the temporal aspects of quality degradation, like VMAF \cite{li2018vmaf}, may provide more accurate predictions of how humans perceive quality.

VMAF (Video Multimethod Assessment Fusion) is a perceptual video quality metric developed by Netflix to evaluate the visual quality of compressed videos. VMAF combines multiple elementary quality metrics, aggregates them through pooling strategies, and uses a machine learning model to estimate human-perceived video quality. It involves four main steps:
\begin{enumerate}
    \item Feature extraction: VMAF extracts low-level features from both the original and compressed videos, focusing on aspects that affect human-perceived video quality. Primary features include VIF (shared visual information), SSIM, ADM (local luminance and contrast deviations), and Motion (which affects compression artefact visibility).
    \item Feature pooling: VMAF segments video frames into non-overlapping blocks and calculates elementary metrics (VIF, SSIM, ADM) for each block. It then aggregates these scores across all blocks in a frame using a pooling strategy (mean or harmonic mean), resulting in a per-frame score for each metric.
    \item Support Vector Regression model: VMAF employs a support vector regression model to combine per-frame elementary metrics into a single quality score. The model is trained on a dataset of videos annotated with subjective quality scores, reflecting human opinions.
    \item Final VMAF score: The support vector regression model outputs a per-frame VMAF score representing perceived video quality. These per-frame scores are typically pooled (using the mean) to compute a single VMAF score for the entire video.
\end{enumerate}

% Another approach that enphasizes the importance of edges in restored images is the ERQA metric, introduced in \cite{kirillova2021erqa}, and improved in \cite{lyapustin2022towards}, that is based on the Canny edge detector algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \input{sections/psnr}
% \input{sections/ssim}
% \input{sections/ms-ssim}
% \input{sections/lpips-comp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
