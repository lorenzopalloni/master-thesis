\myChapter{Quality Metrics}
\label{chap:metrics}

% TODOs
% - DONE - explain PSNR
% - explain SSIM
% - explain MS-SSIM
% - explain LPIPS
% - explain DISTS
% - explain ERQA
% - explain SFSN
% - explain MOS, 2AFC, JND

% An overview of the most used quality assessment metrics in image restoration tasks.

% This work focuses on frame super-resolution and compression artifact removal in real-time videos.
% The former aims to increase both resolution and quality of a given frame, while the latter to remove artifacts that are commonly generated by lossy compression algorithms such as JPEG, JPEG-2000, and BPG for images and H.264/H.265 for videos.

% - mean opinion score (MOS)
% - two alternative force choices (2AFC)
% - Just Noticeable Difference (JND)

It is difficult to define an objective quantification of results from super-resolution and compression artifact removal operations that reflects human judgement.

Plenty of quality metrics have been proposed in the literature.
Traditional metrics, such as pixel-wise mean squared error (MSE), Peak Signal-to-Noise Ratio (PSNR) \ref{sec:psnr}, Structural SIMilarity (SSIM) \cite{wang2004image}, and Multi-Scale SSIM (MS-SSIM) \cite{wang2003multiscale} have begun to be considered inadequate for Image Quality Assessment (IQA) in recent years.

A promising line of work is based on deep-learning features extracted from well-trained CNN models like AlexNet and VGG-16.
FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, LPIPS-Comp \cite{patel2021saliency}, E-LPIPS \cite{kettunen2019lpips}, and DISTS \cite{ding2020image} are all examples of deep-learning-based metrics.

Another approach that enphasizes the importance of edges in restored images is ERQA, introduced in \cite{kirillova2021erqa}, and improved in \cite{lyapustin2022towards}, a metric based on the Canny edge detector.
- SFSN (Structural Fidelity versus Statistical naturalness \cite{zhou2021image}
So far, only Full-Reference IQA (FR-IQA) measures have been mentioned, but No-Reference IQA (NR-IQA) metrics - that evaluate processed images without their originals - are commonly used, and some examples are BRISQUE, NIQE, PIQE, and CONTRIQUE.


LIVE, TID2008, CSIQ, TID2013 are examples of FR-IQA datasets, while AVA, LIVE In the Wild are NR-IQA datasets, that is they assess the quality of an image by itself, without a reference image.

\section{PSNR}
\label{sec:psnr}
Let $y$ be an image and $\hat{y}$ be a distorted version of $y$.
A measure of similarity between the two given images is the Peak Signal-to-Noise Ratio:

\begin{align}
    PSNR(y, \hat{y}) \coloneqq 10 \cdot \log_{10} \left\{ \frac{MAX(y)^2}{\sqrt{MSE(y, \hat{y})}} \right\},
\end{align}

where $MAX(y)$ is the maximum value of $y$, and $MSE(y, \hat{y}) \coloneqq || y - \hat{y} ||^2_2$.

\section{SSIM}
\label{sec:ssim}
Structural SIMilarity (SSIM) is a signal-based quality metric...

\section{LPIPS}
\label{sec:lpips}
Learned Perceptual Image Patch Similarity (LPIPS) is a deep-learning based metric...


%  Traditional similarity met-
% rics such as PSNR and SSIM (Wang et al., 2004) are
% often used to evaluate super-resolution models, but
% they yield poor results and are unstable when deal-
% ing with shifts and other common super-resolution ar-
% tifacts. LPIPS (Zhang et al., 2018) is increasingly
% popular for this task, but it originally aimed to as-
% sess perceptual similarity rather than fidelity. The new
% DISTS (Ding et al., 2020a) metric is an improvement
% on LPIPS, but it also focuses on perceptual similarity.
% 
% - while you read the selected 4 papers, write the following things:
% - describe main objectives: super-resolution and compression artifact removal
% - main metrics Full-Reference: LPIPS, SSIM, PSNR
% - main metrics No-Reference: Contrique, VMAF??
% - improvements of LPIPS: ERQA, saliency (amazon.science)
% 
% - No-ref vs full-ref
% 
% - LPIPS
% - DISTS
% - SSIM (Structural SIMilarity)
% - MS-SSIM (Multi-scale SSIM)
% - MSE
% - PSNR (Peak Signal-to-Noise Ratio)
% 
% - ERQAv1  % in this paper there is an interesting barplot of the most used QA metrics
%     - Edge-Restoration Quality Assessment for Video Super-Resolution
% - ERQAv2
% 
% - E-LPIPS (ensemble LPIPS)
% - VMAF
% 
% % two popular no-reference metrics for images.
% - NIQE
% - BRISQUE 
% 
% % language based image quality assessment
% - In the article "Language Based Image Quality Assessment", the authors claim that a fine grained semantic computer vision task can be a great proxy for human level image judgement.
% 
% - MOS (Mean Opinion Score)
% 
% - NoGAN approach
%     - Image and Video Restoration and Compression Artefact Removal Using a NoGAN Approach 
%     - method developed in DeOldify, then used in https://www.fast.ai/2019/05/03/decrappify/ 
%     - NoGAN is a method to train a GAN architecture to obtain better
%         results and stabilizing training and generation of images. The
%         main idea is to pre-train generator and discriminator separately
%         and then perform a final adversarial training step as is performed in
%         standard GANs. In this setting the generator is initially trained using
%         some perceptual loss, then the generated “fake" images are used
%         to train the discriminator as a binary classifier.
%     - Both full-reference
%         image quality metrics (i.e. metrics that compare a processed image
%         to the original high-quality image – SSIM [ 13] and LPIPS [15]) and
%         no-reference metrics (i.e. metrics that evaluate the naturalness of
%         an image – BRISQUE [11] and NIQE [12 ]) have been used to eval-
%         uate the performance of the system and the effectiveness of the
%         perceptual loss and of the final GAN training step.
% 
% - U-Net
% - SR-UNet
%     - can be used to perform super resolution and compression artifact removal in videos.
%     - effectiveness of SR-UNet demostrated using:
%         - signal-based scores such as VMAF
%         - perceptual-based scores such as LPIPS
% 
% 
% When dealing with image restoration tasks, a reference image is
% often available to perform evaluation. Full-reference image qual-
% ity assessment is an evaluation protocol which uses a reference
% version of an image to compute a similarity. Popular metrics are
% Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE).
% 
