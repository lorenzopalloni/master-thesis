\myChapter{Metrics}
\label{chap:Metrics}

To evaluate the performance of deep learning models for super-resolution, various metrics have been proposed in the current literature, which can be broadly categorized into traditional and perceptual metrics.

\section{Traditional Metrics}
\label{sec:traditional-metrics}
Traditional metrics are based on simple numerical comparisons between the generated and ground-truth images. Some commonly used traditional metrics include PSNR, MSE, SSIM \cite{wang2004image}, and MS-SSIM \cite{wang2003multiscale}.

PSNR (Peak Signal-to-Noise Ratio) is a widely used metric that measures the ratio of the peak signal power to the noise power in an image. It is calculated as the logarithm of the ratio of the maximum possible pixel value to the mean squared error between the predicted and ground-truth images. However, PSNR has been criticized for not being a reliable measure of image quality, as it does not correlate well with human perception.

MSE (Mean Squared Error) measures the average squared difference between the predicted and ground-truth images, with lower values indicating better image quality. However, like PSNR, it has been found to poorly correlate with human perception.

SSIM (Structural SIMilarity) is a more sophisticated metric that takes into account both structural information and pixel values in the image. It measures the similarity between the predicted and ground-truth images based on their luminance, contrast, and structure, and has been found to better correlate with human perception than PSNR and MSE.

MS-SSIM (Multi-Scale SSIM) is an improved version of the SSIM metric that takes into account the multi-scale nature of the human visual system.

\section{Perceptual Metrics}
\label{sec:perceptual-metrics}
Perceptual metrics aim to evaluate image quality based on human perception, measuring the visual similarity between predicted and ground-truth images, rather than simply their pixel-wise differences. Examples of deep learning-based perceptual metrics include FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, LPIPS-Comp \cite{patel2021saliency}, E-LPIPS \cite{kettunen2019lpips}, and DISTS \cite{ding2020image}.

FID (Fréchet Inception Distance) is a perceptual metric used to evaluate the similarity between two sets of images by measuring the distance between their feature representations obtained from a pre-trained neural network.

LPIPS (Learned Perceptual Image Patch Similarity) computes the similarity between two images based on their perceptual similarity at the patch level, using a deep neural network trained on human perceptual judgments.

LPIPS-Comp (LPIPS with Saliency Map Comparison) is an extension of LPIPS that incorporates saliency maps to enhance the metric's sensitivity to the most salient regions in the image.

E-LPIPS (Ensembled LPIPS) is an improved version of the LPIPS metric that employs an ensemble of neural networks trained on different subsets of images to improve the stability and robustness of the metric.

DISTS (Deep Image Structure and Texture Similarity) is based on a Siamese neural network, which takes two input images, and extracts features from them. These features are then compared at multiple levels to compute a final similarity score between the the two images.

Other perceptual metrics such as MOS (Mean Opinion Score), 2AFC (Two Alternative Forced Choice), and JND (Just Noticeable Difference) are all examples of perceptual metrics that are based on subjective human evaluations of image quality.

MOS involves asking human subjects to rate the quality of the predicted images on a scale from 1 to 5, with the MOS score calculated as the average of these ratings. 2AFC involves presenting two images to human subjects and asking them to choose the one that appears to be of higher quality, while JND involves asking subjects to identify the minimum perceptible difference between two images.

Overall, the choice of metric for evaluating super-resolution models depends on the specific application and the goals of the study. Traditional metrics such as PSNR, MSE, and SSIM are simple to compute and provide a good baseline for comparison. Perceptual metrics such as LPIPS and MOS provide a more accurate measure of human perception, but are more complex to compute and require additional resources. A combination of both traditional and perceptual metrics can provide a comprehensive evaluation of the performance of super-resolution models.

While Full-Reference IQA (FR-IQA) measures have been discussed thus far, it is worth noting that No-Reference IQA (NR-IQA) metrics are also commonly employed to assess processed images without their original counterparts. Examples of such NR-IQA metrics include BRISQUE, NIQE, PIQE, and CONTRIQUE. Datasets like LIVE, TID2008, CSIQ, and TID2013 are considered Full-Reference IQA (FR-IQA) datasets, since they need a reference image to assess the image quality. In contrast, AVA and LIVE In the Wild are No-Reference IQA (NR-IQA) datasets, where image quality is evaluated independently without the necessity of a reference image.

% These perceptual metrics are commonly used in the evaluation of super-resolution models, in addition to deep learning-based perceptual metrics. MOS is often used as a benchmark for evaluating the visual quality of predicted images, while 2AFC and JND are used to measure the difference in visual quality between predicted and ground-truth images. Together, these metrics provide a comprehensive evaluation of the visual quality of predicted images from different perspectives.

% Another approach that enphasizes the importance of edges in restored images is the ERQA metric, introduced in \cite{kirillova2021erqa}, and improved in \cite{lyapustin2022towards}, that is based on the Canny edge detector algorithm.

% Super-resolution (SR) with deep learning models has achieved impressive quantitative performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics. However, improving the perceptual quality of the generated images remains a challenge, especially when using quantitative metrics such as Learned Perceptual Image Patch Similarity (LPIPS).
% 
% One of the main challenges in improving the perceptual quality of SR images is to preserve fine-grained details while avoiding artifacts and noise amplification. Deep learning models for SR can generate images with high-frequency details, but these details may not be consistent with the true high-resolution images, leading to a loss of perceptual quality. This can be exacerbated when training datasets do not contain enough diverse and realistic high-resolution images.
% 
% Another challenge in SR with deep learning models is the balance between overfitting and underfitting. Overfitting occurs when the model memorizes the training data and fails to generalize to new data, resulting in poor performance on the validation or test sets. On the other hand, underfitting occurs when the model is too simple to capture the complex mapping between low-resolution and high-resolution images, resulting in poor performance on both the training and validation sets.
% 
% In addition to these challenges, the use of quantitative metrics such as LPIPS to evaluate the perceptual quality of SR images has its limitations. LPIPS is a learned distance metric that measures the perceptual difference between two images based on the feature representations of a pre-trained deep neural network. While LPIPS can capture some aspects of human perception, it is not a perfect measure of image quality and may not fully capture all the nuances of perceptual quality.
% 
% Despite these challenges, ongoing research is addressing these issues in SR with deep learning models. For example, recent studies have proposed novel loss functions and network architectures to improve the perceptual quality of SR images, while others have explored more diverse and realistic training datasets to improve generalization. In addition, alternative evaluation metrics such as human perception studies can provide a more comprehensive assessment of perceptual quality.

% This chapter investigates various Image Quality Assessment (IQA) metrics commonly used in the current literature to evaluate results and improve performance of super-resolution and compression artifact removal operations.

% For our purpose, a metric is as good as how much it agrees with the average human judgement. The agreement is often measured with Pearson, Kendall, and Spearman correlation coefficients, while the average human judgement is estimated using sets of images structured for Mean Opinion Score (MOS), Two Alternative Force Choices (2AFC), or Just Noticeable Difference (JND) approaches.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \input{sections/psnr}
% \input{sections/ssim}
% \input{sections/ms-ssim}
% \input{sections/lpips-comp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{DISTS}
% \label{sec:dists}
% The authors of DISTS \cite{} carried out five major experiments. First, they showed that DISTS has not the best performance overall on LIVE \cite{}, CSIQ \cite{}, and TID2013 \cite{} that are datasets that have been around in the literature long enough to be likely overfitted by recent quality measures. Second, they saw comparable results on the BAPPS dataset against LPIPS (that is a metric trained on the BAPPS dataset). Third, DISTS achieves best performance...

% \section{SFSN}
% \label{sec:sfsn}
% The authors of \cite{zhou2021image} found that a linear combination of a local structural fidelity assessment (SF) and a global statistical naturalness measure (SN) achieves high correlation with human judgement (measured with MOS on public Single Image Super Resolution IQA datasets, such as WIND \cite{yeganeh2015objective}, CVIU \cite{ma2017learning}, and QADS \cite{zhou2019visual}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{SFSN}
% \textbf{Input:} original image $y$; distorted image $\hat{y}$.\\
% \textbf{Output:} SFSN quality score.\\
% \label{alg-kcv}
% \begin{algorithmic}
% 
% \State Convert both $y$ and $\hat{y}$ to grayscale
% \State For efficiency it is possible to scale both images within $[0, 1]$
% \State Get $m$ and $n$, that are respectively the height and the width of $\hat{y}$
% \State $s$ = min(m, n)
% \State Resize $y$ to $s \times s$ $\rightarrow$ $y^{(1)}$
% \State Apply 2D discrete cosine transform to $y^{(1)}$ $\rightarrow$ $y^{(2)}$
% \State Split $y_2$ in low and high frequency $\rightarrow$ $y^{3}_{high}$, $y^{3}_{low}$
% \State Apply the inverse of the 2D discrete cosine transform $\rightarrow$ $y^{4}_{high}$, $y^{4}_{low}$
% 
% \State Resize $\hat{y}$ to $s \times s$ $\rightarrow$ $\hat{y}^{(1)}$
% \State Apply 2D discrete cosine transform to $\hat{y}^{(1)}$ $\rightarrow$ $\hat{y}^{(2)}$
% \State Split $\hat{y}_2$ in low and high frequency $\rightarrow$ $\hat{y}^{3}_{high}$, $\hat{y}^{3}_{low}$
% \State Apply the inverse of the 2D discrete cosine transform $\rightarrow$ $\hat{y}^{4}_{high}$, $\hat{y}^{4}_{low}$
% 
% \State SF = MS-SSIM($y^{(4)_{low}}$, $\hat{y}^{4}_{low}$)
% \State SN = Entropy($y^{(4)}_{high}$)
% 
% \State \Return $(0.9) * SF + 0.1 * SN$  \\ (1 + 0.9) * SF + (1 - 0.9) * SN
% 
% \end{algorithmic}
% \end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% from DISTS paper
% For more than 50 years, the mean squared error (MSE)
% was the standard full-reference method for assessing signal
% fidelity and quality, and it continues to play a fundamental
% role in the development of signal and image processing
% algorithms, despite its poor correlation with human percep-
% tion [1], [2].
% [1] -> Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave
% it? A new look at signal fidelity measures,” IEEE Signal Processing
% Magazine, vol. 26, no. 1, pp. 98–117, 2009.
% 
% [2] -> B. Girod, “What’s wrong with mean-squared error,” in Digital
% Images and Human Vision, A. B. Watson, Ed. The MIT Press, 1993,
% pp. 207–220
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% - ERQAv1  % in this paper there is an interesting barplot of the most used QA metrics
%     - Edge-Restoration Quality Assessment for Video Super-Resolution
% - ERQAv2
%
% - VMAF
% 
% % language based image quality assessment
% - In the article "Language Based Image Quality Assessment", the authors claim that a fine grained semantic computer vision task can be a great proxy for human level image judgement.
% 
% - NoGAN approach
%     - Image and Video Restoration and Compression Artefact Removal Using a NoGAN Approach 
%     - method developed in DeOldify, then used in https://www.fast.ai/2019/05/03/decrappify/ 
%     - NoGAN is a method to train a GAN architecture to obtain better
%         results and stabilizing training and generation of images. The
%         main idea is to pre-train generator and discriminator separately
%         and then perform a final adversarial training step as is performed in
%         standard GANs. In this setting the generator is initially trained using
%         some perceptual loss, then the generated “fake" images are used
%         to train the discriminator as a binary classifier.
%     - Both full-reference
%         image quality metrics (i.e. metrics that compare a processed image
%         to the original high-quality image – SSIM [ 13] and LPIPS [15]) and
%         no-reference metrics (i.e. metrics that evaluate the naturalness of
%         an image – BRISQUE [11] and NIQE [12 ]) have been used to eval-
%         uate the performance of the system and the effectiveness of the
%         perceptual loss and of the final GAN training step.
% 
% - U-Net
% - SR-UNet
%     - can be used to perform super resolution and compression artifact removal in videos.
%     - effectiveness of SR-UNet demostrated using:
%         - signal-based scores such as VMAF
%         - perceptual-based scores such as LPIPS
% 
