\myChapter{Quality Metrics}
\label{chap:metrics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Random sentences:
% An overview of the most used quality assessment metrics in image restoration tasks.

% This work focuses on frame super-resolution and compression artifact removal in real-time videos.
% The former aims to increase both resolution and quality of a given frame, while the latter to remove artifacts that are commonly generated by lossy compression algorithms such as JPEG, JPEG-2000, and BPG for images and H.264/H.265 for videos.

% - mean opinion score (MOS)
% - two alternative force choices (2AFC)
% - Just Noticeable Difference (JND)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter investigates various Image Quality Assessment (IQA) metrics commonly used in the current literature to evaluate results and improve performance of super-resolution and compression artifact removal operations.
For our purpose, a metric is as good as how much it agrees with the average human judgement.
The agreement is often measured with Pearson, Kendall, and Spearman correlation coefficients, while the average human judgement is estimated using sets of images structured for Mean Opinion Score (MOS), Two Alternative Force Choices (2AFC), or Just Noticeable Difference (JND) approaches.

% Not sure about this

It is difficult to define an objective quantification of results from super-resolution and compression artifact removal operations that reflects human judgement.

Plenty of quality metrics have been proposed in the literature.

Traditional metrics, such as Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) \ref{sec:psnr}, Structural Similarity (SSIM) \cite{wang2004image}, and Multi-Scale SSIM (MS-SSIM) \cite{wang2003multiscale} have begun to be considered inadequate for Image Quality Assessment (IQA) in recent years.

A promising line of work is based on deep-learning features extracted from well-trained CNN models like AlexNet and VGG-16.
FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, LPIPS-Comp \cite{patel2021saliency}, E-LPIPS \cite{kettunen2019lpips}, and DISTS \cite{ding2020image} are all examples of deep-learning-based metrics.

Another approach that enphasizes the importance of edges in restored images is ERQA, introduced in \cite{kirillova2021erqa}, and improved in \cite{lyapustin2022towards}, a metric based on the Canny edge detector.
- SFSN (Structural Fidelity versus Statistical naturalness \cite{zhou2021image}
So far, only Full-Reference IQA (FR-IQA) measures have been mentioned, but No-Reference IQA (NR-IQA) metrics - that evaluate processed images without their originals - are commonly used, and some examples are BRISQUE, NIQE, PIQE, and CONTRIQUE.


LIVE, TID2008, CSIQ, TID2013 are examples of FR-IQA datasets, while AVA, LIVE In the Wild are NR-IQA datasets, that is they assess the quality of an image by itself, without a reference image.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/psnr}
\input{sections/ssim}
\input{sections/ms-ssim}
\input{sections/lpips-comp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DISTS}
\label{sec:dists}
The authors of DISTS \cite{} carried out five major experiments. First, they showed that DISTS has not the best performance overall on LIVE \cite{}, CSIQ \cite{}, and TID2013 \cite{} that are datasets that have been around in the literature long enough to be likely overfitted by recent quality measures. Second, they saw comparable results on the BAPPS dataset against LPIPS (that is a metric trained on the BAPPS dataset). Third, DISTS achieves best performance...

\section{SFSN}
\label{sec:sfsn}
The authors of \cite{zhou2021image} found that a linear combination of a local structural fidelity assessment (SF) and a global statistical naturalness measure (SN) achieves high correlation with human judgement (measured with MOS on public Single Image Super Resolution IQA datasets, such as WIND \cite{yeganeh2015objective}, CVIU \cite{ma2017learning}, and QADS \cite{zhou2019visual}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{SFSN}
% \textbf{Input:} original image $y$; distorted image $\hat{y}$.\\
% \textbf{Output:} SFSN quality score.\\
% \label{alg-kcv}
% \begin{algorithmic}
% 
% \State Convert both $y$ and $\hat{y}$ to grayscale
% \State For efficiency it is possible to scale both images within $[0, 1]$
% \State Get $m$ and $n$, that are respectively the height and the width of $\hat{y}$
% \State $s$ = min(m, n)
% \State Resize $y$ to $s \times s$ $\rightarrow$ $y^{(1)}$
% \State Apply 2D discrete cosine transform to $y^{(1)}$ $\rightarrow$ $y^{(2)}$
% \State Split $y_2$ in low and high frequency $\rightarrow$ $y^{3}_{high}$, $y^{3}_{low}$
% \State Apply the inverse of the 2D discrete cosine transform $\rightarrow$ $y^{4}_{high}$, $y^{4}_{low}$
% 
% \State Resize $\hat{y}$ to $s \times s$ $\rightarrow$ $\hat{y}^{(1)}$
% \State Apply 2D discrete cosine transform to $\hat{y}^{(1)}$ $\rightarrow$ $\hat{y}^{(2)}$
% \State Split $\hat{y}_2$ in low and high frequency $\rightarrow$ $\hat{y}^{3}_{high}$, $\hat{y}^{3}_{low}$
% \State Apply the inverse of the 2D discrete cosine transform $\rightarrow$ $\hat{y}^{4}_{high}$, $\hat{y}^{4}_{low}$
% 
% \State SF = MS-SSIM($y^{(4)_{low}}$, $\hat{y}^{4}_{low}$)
% \State SN = Entropy($y^{(4)}_{high}$)
% 
% \State \Return $(0.9) * SF + 0.1 * SN$  \\ (1 + 0.9) * SF + (1 - 0.9) * SN
% 
% \end{algorithmic}
% \end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% from DISTS paper
% For more than 50 years, the mean squared error (MSE)
% was the standard full-reference method for assessing signal
% fidelity and quality, and it continues to play a fundamental
% role in the development of signal and image processing
% algorithms, despite its poor correlation with human percep-
% tion [1], [2].
% [1] -> Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave
% it? A new look at signal fidelity measures,” IEEE Signal Processing
% Magazine, vol. 26, no. 1, pp. 98–117, 2009.
% 
% [2] -> B. Girod, “What’s wrong with mean-squared error,” in Digital
% Images and Human Vision, A. B. Watson, Ed. The MIT Press, 1993,
% pp. 207–220
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  Traditional similarity met-
% rics such as PSNR and SSIM (Wang et al., 2004) are
% often used to evaluate super-resolution models, but
% they yield poor results and are unstable when deal-
% ing with shifts and other common super-resolution ar-
% tifacts. LPIPS (Zhang et al., 2018) is increasingly
% popular for this task, but it originally aimed to as-
% sess perceptual similarity rather than fidelity. The new
% DISTS (Ding et al., 2020a) metric is an improvement
% on LPIPS, but it also focuses on perceptual similarity.
% 
% - while you read the selected 4 papers, write the following things:
% - describe main objectives: super-resolution and compression artifact removal
% - main metrics Full-Reference: LPIPS, SSIM, PSNR
% - main metrics No-Reference: Contrique, VMAF??
% - improvements of LPIPS: ERQA, saliency (amazon.science)
% 
% - No-ref vs full-ref
% 
% - LPIPS
% - DISTS
% - SSIM (Structural SIMilarity)
% - MS-SSIM (Multi-scale SSIM)
% - MSE
% - PSNR (Peak Signal-to-Noise Ratio)
% 
% - ERQAv1  % in this paper there is an interesting barplot of the most used QA metrics
%     - Edge-Restoration Quality Assessment for Video Super-Resolution
% - ERQAv2
% 
% - E-LPIPS (ensemble LPIPS)
% - VMAF
% 
% % two popular no-reference metrics for images.
% - NIQE
% - BRISQUE 
% 
% % language based image quality assessment
% - In the article "Language Based Image Quality Assessment", the authors claim that a fine grained semantic computer vision task can be a great proxy for human level image judgement.
% 
% - MOS (Mean Opinion Score)
% 
% - NoGAN approach
%     - Image and Video Restoration and Compression Artefact Removal Using a NoGAN Approach 
%     - method developed in DeOldify, then used in https://www.fast.ai/2019/05/03/decrappify/ 
%     - NoGAN is a method to train a GAN architecture to obtain better
%         results and stabilizing training and generation of images. The
%         main idea is to pre-train generator and discriminator separately
%         and then perform a final adversarial training step as is performed in
%         standard GANs. In this setting the generator is initially trained using
%         some perceptual loss, then the generated “fake" images are used
%         to train the discriminator as a binary classifier.
%     - Both full-reference
%         image quality metrics (i.e. metrics that compare a processed image
%         to the original high-quality image – SSIM [ 13] and LPIPS [15]) and
%         no-reference metrics (i.e. metrics that evaluate the naturalness of
%         an image – BRISQUE [11] and NIQE [12 ]) have been used to eval-
%         uate the performance of the system and the effectiveness of the
%         perceptual loss and of the final GAN training step.
% 
% - U-Net
% - SR-UNet
%     - can be used to perform super resolution and compression artifact removal in videos.
%     - effectiveness of SR-UNet demostrated using:
%         - signal-based scores such as VMAF
%         - perceptual-based scores such as LPIPS
% 
% 
% When dealing with image restoration tasks, a reference image is
% often available to perform evaluation. Full-reference image qual-
% ity assessment is an evaluation protocol which uses a reference
% version of an image to compute a similarity. Popular metrics are
% Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE).
% 
