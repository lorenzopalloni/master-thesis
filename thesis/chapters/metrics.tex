\myChapter{Metrics}
\label{chap:Metrics}

To evaluate the performance of deep learning models for super-resolution, various metrics have been proposed in the current literature, which can be broadly categorized into traditional and perceptual metrics.

Traditional metrics are based on simple numerical comparisons between the generated and ground-truth images. Some commonly used traditional metrics include peak signal-to-noise ratio (PSNR) \cref{sec:psnr}, mean squared error (MSE), structural similarity index (SSIM) \cite{wang2004image}, multi-scale SSIM (MS-SSIM) \cite{wang2003multiscale}.

% PSNR is a widely used metric that measures the ratio of the peak signal power to the noise power in an image. It is calculated as the logarithm of the ratio of the maximum possible pixel value to the mean squared error between the predicted and ground-truth images. However, PSNR has been criticized for not being a reliable measure of image quality, as it does not correlate well with human perception.

% MSE measures the average squared difference between the predicted and ground-truth images, with lower values indicating better image quality. However, like PSNR, it has been found to poorly correlate with human perception.

% SSIM is a more sophisticated metric that takes into account both structural information and pixel values in the image. It measures the similarity between the predicted and ground-truth images based on their luminance, contrast, and structure, and has been found to better correlate with human perception than PSNR and MSE.

Perceptual metrics aim to evaluate image quality based on human perception, measuring the visual similarity between predicted and ground-truth images, rather than simply their pixel-wise differences. Examples of deep learning-based perceptual metrics include FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, LPIPS-Comp \cite{patel2021saliency}, E-LPIPS \cite{kettunen2019lpips}, and DISTS \cite{ding2020image}. Other perceptual metrics such as MOS (Mean Opinion Score), 2AFC (Two Alternative Forced Choice), and JND (Just Noticeable Difference) are all examples of perceptual metrics that are based on subjective human evaluations of image quality. MOS involves asking human subjects to rate the quality of the predicted images on a scale from 1 to 5, with the MOS score calculated as the average of these ratings. 2AFC involves presenting two images to human subjects and asking them to choose the one that appears to be of higher quality, while JND involves asking subjects to identify the minimum perceptible difference between two images. These perceptual metrics are commonly used in the evaluation of super-resolution models, in addition to deep learning-based perceptual metrics. MOS is often used as a benchmark for evaluating the visual quality of predicted images, while 2AFC and JND are used to measure the difference in visual quality between predicted and ground-truth images. Together, these metrics provide a comprehensive evaluation of the visual quality of predicted images from different perspectives.

>>> ALL GOOD SO FAR, more or less

One commonly used perceptual metric is the LPIPS metric (Learned Perceptual Image Patch Similarity). LPIPS is a distance metric that is based on a pre-trained neural network, which has been trained to predict human perceptual similarity between image patches. The LPIPS metric has been found to better correlate with human perception than traditional metrics, making it a useful tool for evaluating super-resolution models.

Another popular perceptual metric is the mean opinion score (MOS), which is based on subjective evaluations by human observers. In MOS tests, human subjects are asked to rate the visual quality of the predicted images on a scale from 1 to 5. The MOS score is then calculated as the average of these ratings. MOS tests are time-consuming and expensive to conduct, but they provide a valuable measure of human perception.

Another approach that enphasizes the importance of edges in restored images is ERQA, introduced in \cite{kirillova2021erqa}, and improved in \cite{lyapustin2022towards}, a metric based on the Canny edge detector algorithm.

Overall, the choice of metric for evaluating super-resolution models depends on the specific application and the goals of the study. Traditional metrics such as PSNR, MSE, and SSIM are simple to compute and provide a good baseline for comparison. Perceptual metrics such as LPIPS and MOS provide a more accurate measure of human perception, but are more complex to compute and require additional resources. A combination of both traditional and perceptual metrics can provide a comprehensive evaluation of the performance of super-resolution models.

HOLA

% Super-resolution (SR) with deep learning models has achieved impressive quantitative performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics. However, improving the perceptual quality of the generated images remains a challenge, especially when using quantitative metrics such as Learned Perceptual Image Patch Similarity (LPIPS).
% 
% One of the main challenges in improving the perceptual quality of SR images is to preserve fine-grained details while avoiding artifacts and noise amplification. Deep learning models for SR can generate images with high-frequency details, but these details may not be consistent with the true high-resolution images, leading to a loss of perceptual quality. This can be exacerbated when training datasets do not contain enough diverse and realistic high-resolution images.
% 
% Another challenge in SR with deep learning models is the balance between overfitting and underfitting. Overfitting occurs when the model memorizes the training data and fails to generalize to new data, resulting in poor performance on the validation or test sets. On the other hand, underfitting occurs when the model is too simple to capture the complex mapping between low-resolution and high-resolution images, resulting in poor performance on both the training and validation sets.
% 
% In addition to these challenges, the use of quantitative metrics such as LPIPS to evaluate the perceptual quality of SR images has its limitations. LPIPS is a learned distance metric that measures the perceptual difference between two images based on the feature representations of a pre-trained deep neural network. While LPIPS can capture some aspects of human perception, it is not a perfect measure of image quality and may not fully capture all the nuances of perceptual quality.
% 
% Despite these challenges, ongoing research is addressing these issues in SR with deep learning models. For example, recent studies have proposed novel loss functions and network architectures to improve the perceptual quality of SR images, while others have explored more diverse and realistic training datasets to improve generalization. In addition, alternative evaluation metrics such as human perception studies can provide a more comprehensive assessment of perceptual quality.

% This chapter investigates various Image Quality Assessment (IQA) metrics commonly used in the current literature to evaluate results and improve performance of super-resolution and compression artifact removal operations.

% For our purpose, a metric is as good as how much it agrees with the average human judgement. The agreement is often measured with Pearson, Kendall, and Spearman correlation coefficients, while the average human judgement is estimated using sets of images structured for Mean Opinion Score (MOS), Two Alternative Force Choices (2AFC), or Just Noticeable Difference (JND) approaches.

It is difficult to define an objective quantification of results from super-resolution and compression artifact removal operations that reflects human judgement.

Plenty of quality metrics have been proposed in the literature.

Traditional metrics, such as Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) \ref{sec:psnr}, Structural Similarity (SSIM) \cite{wang2004image}, and Multi-Scale SSIM (MS-SSIM) \cite{wang2003multiscale} have begun to be considered inadequate for Image Quality Assessment (IQA) in recent years.

A promising line of work is based on deep-learning features extracted from well-trained CNN models like AlexNet and VGG-16.
FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, LPIPS-Comp \cite{patel2021saliency}, E-LPIPS \cite{kettunen2019lpips}, and DISTS \cite{ding2020image} are all examples of deep-learning-based metrics.

Another approach that enphasizes the importance of edges in restored images is ERQA, introduced in \cite{kirillova2021erqa}, and improved in \cite{lyapustin2022towards}, a metric based on the Canny edge detector algorithm.

% - SFSN (Structural Fidelity versus Statistical naturalness \cite{zhou2021image}

So far, only Full-Reference IQA (FR-IQA) measures have been mentioned, but No-Reference IQA (NR-IQA) metrics - that evaluate processed images without their originals - are commonly used, and some examples are BRISQUE, NIQE, PIQE, and CONTRIQUE.


LIVE, TID2008, CSIQ, TID2013 are examples of FR-IQA datasets, while AVA, LIVE In the Wild are NR-IQA datasets, that is they assess the quality of an image by itself, without a reference image.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/psnr}
\input{sections/ssim}
\input{sections/ms-ssim}
\input{sections/lpips-comp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{DISTS}
% \label{sec:dists}
% The authors of DISTS \cite{} carried out five major experiments. First, they showed that DISTS has not the best performance overall on LIVE \cite{}, CSIQ \cite{}, and TID2013 \cite{} that are datasets that have been around in the literature long enough to be likely overfitted by recent quality measures. Second, they saw comparable results on the BAPPS dataset against LPIPS (that is a metric trained on the BAPPS dataset). Third, DISTS achieves best performance...

% \section{SFSN}
% \label{sec:sfsn}
% The authors of \cite{zhou2021image} found that a linear combination of a local structural fidelity assessment (SF) and a global statistical naturalness measure (SN) achieves high correlation with human judgement (measured with MOS on public Single Image Super Resolution IQA datasets, such as WIND \cite{yeganeh2015objective}, CVIU \cite{ma2017learning}, and QADS \cite{zhou2019visual}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{SFSN}
% \textbf{Input:} original image $y$; distorted image $\hat{y}$.\\
% \textbf{Output:} SFSN quality score.\\
% \label{alg-kcv}
% \begin{algorithmic}
% 
% \State Convert both $y$ and $\hat{y}$ to grayscale
% \State For efficiency it is possible to scale both images within $[0, 1]$
% \State Get $m$ and $n$, that are respectively the height and the width of $\hat{y}$
% \State $s$ = min(m, n)
% \State Resize $y$ to $s \times s$ $\rightarrow$ $y^{(1)}$
% \State Apply 2D discrete cosine transform to $y^{(1)}$ $\rightarrow$ $y^{(2)}$
% \State Split $y_2$ in low and high frequency $\rightarrow$ $y^{3}_{high}$, $y^{3}_{low}$
% \State Apply the inverse of the 2D discrete cosine transform $\rightarrow$ $y^{4}_{high}$, $y^{4}_{low}$
% 
% \State Resize $\hat{y}$ to $s \times s$ $\rightarrow$ $\hat{y}^{(1)}$
% \State Apply 2D discrete cosine transform to $\hat{y}^{(1)}$ $\rightarrow$ $\hat{y}^{(2)}$
% \State Split $\hat{y}_2$ in low and high frequency $\rightarrow$ $\hat{y}^{3}_{high}$, $\hat{y}^{3}_{low}$
% \State Apply the inverse of the 2D discrete cosine transform $\rightarrow$ $\hat{y}^{4}_{high}$, $\hat{y}^{4}_{low}$
% 
% \State SF = MS-SSIM($y^{(4)_{low}}$, $\hat{y}^{4}_{low}$)
% \State SN = Entropy($y^{(4)}_{high}$)
% 
% \State \Return $(0.9) * SF + 0.1 * SN$  \\ (1 + 0.9) * SF + (1 - 0.9) * SN
% 
% \end{algorithmic}
% \end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% from DISTS paper
% For more than 50 years, the mean squared error (MSE)
% was the standard full-reference method for assessing signal
% fidelity and quality, and it continues to play a fundamental
% role in the development of signal and image processing
% algorithms, despite its poor correlation with human percep-
% tion [1], [2].
% [1] -> Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave
% it? A new look at signal fidelity measures,” IEEE Signal Processing
% Magazine, vol. 26, no. 1, pp. 98–117, 2009.
% 
% [2] -> B. Girod, “What’s wrong with mean-squared error,” in Digital
% Images and Human Vision, A. B. Watson, Ed. The MIT Press, 1993,
% pp. 207–220
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  Traditional similarity met-
% rics such as PSNR and SSIM (Wang et al., 2004) are
% often used to evaluate super-resolution models, but
% they yield poor results and are unstable when deal-
% ing with shifts and other common super-resolution ar-
% tifacts. LPIPS (Zhang et al., 2018) is increasingly
% popular for this task, but it originally aimed to as-
% sess perceptual similarity rather than fidelity. The new
% DISTS (Ding et al., 2020a) metric is an improvement
% on LPIPS, but it also focuses on perceptual similarity.
% 
% - while you read the selected 4 papers, write the following things:
% - describe main objectives: super-resolution and compression artifact removal
% - main metrics Full-Reference: LPIPS, SSIM, PSNR
% - main metrics No-Reference: Contrique, VMAF??
% - improvements of LPIPS: ERQA, saliency (amazon.science)
% 
% - No-ref vs full-ref
% 
% - LPIPS
% - DISTS
% - SSIM (Structural SIMilarity)
% - MS-SSIM (Multi-scale SSIM)
% - MSE
% - PSNR (Peak Signal-to-Noise Ratio)
% 
% - ERQAv1  % in this paper there is an interesting barplot of the most used QA metrics
%     - Edge-Restoration Quality Assessment for Video Super-Resolution
% - ERQAv2
% 
% - E-LPIPS (ensemble LPIPS)
% - VMAF
% 
% % two popular no-reference metrics for images.
% - NIQE
% - BRISQUE 
% 
% % language based image quality assessment
% - In the article "Language Based Image Quality Assessment", the authors claim that a fine grained semantic computer vision task can be a great proxy for human level image judgement.
% 
% - MOS (Mean Opinion Score)
% 
% - NoGAN approach
%     - Image and Video Restoration and Compression Artefact Removal Using a NoGAN Approach 
%     - method developed in DeOldify, then used in https://www.fast.ai/2019/05/03/decrappify/ 
%     - NoGAN is a method to train a GAN architecture to obtain better
%         results and stabilizing training and generation of images. The
%         main idea is to pre-train generator and discriminator separately
%         and then perform a final adversarial training step as is performed in
%         standard GANs. In this setting the generator is initially trained using
%         some perceptual loss, then the generated “fake" images are used
%         to train the discriminator as a binary classifier.
%     - Both full-reference
%         image quality metrics (i.e. metrics that compare a processed image
%         to the original high-quality image – SSIM [ 13] and LPIPS [15]) and
%         no-reference metrics (i.e. metrics that evaluate the naturalness of
%         an image – BRISQUE [11] and NIQE [12 ]) have been used to eval-
%         uate the performance of the system and the effectiveness of the
%         perceptual loss and of the final GAN training step.
% 
% - U-Net
% - SR-UNet
%     - can be used to perform super resolution and compression artifact removal in videos.
%     - effectiveness of SR-UNet demostrated using:
%         - signal-based scores such as VMAF
%         - perceptual-based scores such as LPIPS
% 
% 
% When dealing with image restoration tasks, a reference image is
% often available to perform evaluation. Full-reference image qual-
% ity assessment is an evaluation protocol which uses a reference
% version of an image to compute a similarity. Popular metrics are
% Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE).
% 
